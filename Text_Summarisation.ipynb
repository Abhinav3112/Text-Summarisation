{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Summarization using TF/IDF and TextRank Algorithm"
      ],
      "metadata": {
        "id": "g2MoyaooKAXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF"
      ],
      "metadata": {
        "id": "aj3YWKr3a0AL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-I7kgUWP2yS"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "import math\n",
        "import re\n",
        "import operator\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-AlltDgQbx6",
        "outputId": "cc8b0c55-6fa8-4d2b-d3cd-70d0a6e561f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing WordNetLemmatizer"
      ],
      "metadata": {
        "id": "FeAOlOCpQgnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "Stopwords = set(stopwords.words('english'))\n",
        "wordlemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PxFfUgKQekx",
        "outputId": "3e59f52c-763c-4827-e69b-811a549f0432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text preprocessing"
      ],
      "metadata": {
        "id": "ePD_tU6_Qrn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization\n",
        "def lemmatize_words(words):\n",
        "    lemmatized_words = []\n",
        "    for word in words:\n",
        "        lemmatized_words.append(wordlemmatizer.lemmatize(word))\n",
        "    return lemmatized_words"
      ],
      "metadata": {
        "id": "3-qkB18nQuhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To Remove special characters\n",
        "def remove_special_characters(text):\n",
        "    regex = r'[^a-zA-Z0-9\\s]'\n",
        "    text = re.sub(regex,'',text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "Ph50xhA-Qx_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\tLowercasing\n",
        "def freq(words):\n",
        "    words = [word.lower() for word in words]\n",
        "    dict_freq = {}\n",
        "    words_unique = []\n",
        "    for word in words:\n",
        "        if word not in words_unique:\n",
        "            words_unique.append(word)\n",
        "    for word in words_unique:\n",
        "        dict_freq[word] = words.count(word)\n",
        "    return dict_freq"
      ],
      "metadata": {
        "id": "8lwf6nM1Q0jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\tPOS Tagging\n",
        "def pos_tagging(text):\n",
        "    pos_tag = nltk.pos_tag(text.split())\n",
        "    pos_tagged_noun_verb = []\n",
        "    for word,tag in pos_tag:\n",
        "        if tag == \"NN\" or tag == \"NNP\" or tag == \"NNS\" or tag == \"VB\" or tag == \"VBD\" or tag == \"VBG\" or tag == \"VBN\" or tag == \"VBP\" or tag == \"VBZ\":\n",
        "             pos_tagged_noun_verb.append(word)\n",
        "    return pos_tagged_noun_verb"
      ],
      "metadata": {
        "id": "zAfufRUAQ6yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF score functions"
      ],
      "metadata": {
        "id": "oDquHzWqRAf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF Score function\n",
        "def tf_score(word,sentence):\n",
        "    freq_sum = 0\n",
        "    word_frequency_in_sentence = 0\n",
        "    len_sentence = len(sentence)\n",
        "    for word_in_sentence in sentence.split():\n",
        "        if word == word_in_sentence:\n",
        "            word_frequency_in_sentence = word_frequency_in_sentence + 1\n",
        "    tf =  word_frequency_in_sentence/ len_sentence\n",
        "    return tf"
      ],
      "metadata": {
        "id": "QG1GU6fmQ9o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IDF Score function\n",
        "def idf_score(no_of_sentences,word,sentences):\n",
        "    no_of_sentence_containing_word = 0\n",
        "    for sentence in sentences:\n",
        "        sentence = remove_special_characters(str(sentence))\n",
        "        sentence = re.sub(r'\\d+', '', sentence)\n",
        "        sentence = sentence.split()\n",
        "        sentence = [word for word in sentence if word.lower() not in Stopwords and len(word)>1]\n",
        "        sentence = [word.lower() for word in sentence]\n",
        "        sentence = [wordlemmatizer.lemmatize(word) for word in sentence]\n",
        "        if word in sentence:\n",
        "            no_of_sentence_containing_word = no_of_sentence_containing_word + 1\n",
        "    idf = math.log10(no_of_sentences/no_of_sentence_containing_word)\n",
        "    return idf"
      ],
      "metadata": {
        "id": "wp8hhyfiQ_2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF function\n",
        "def word_tfidf(dict_freq,word,sentences,sentence):\n",
        "    word_tfidf = []\n",
        "    tf = tf_score(word,sentence)\n",
        "    idf = idf_score(len(sentences),word,sentences)\n",
        "    return tf*idf"
      ],
      "metadata": {
        "id": "kfymjbKkRKq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding most important sentences and Generating summary"
      ],
      "metadata": {
        "id": "TXn6ZoUORYAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_importance(sentence,dict_freq,sentences):\n",
        "    sentence_score = 0\n",
        "    sentence = remove_special_characters(str(sentence)) \n",
        "    sentence = re.sub(r'\\d+', '', sentence)\n",
        "    pos_tagged_sentence = [] \n",
        "    no_of_sentences = len(sentences)\n",
        "    pos_tagged_sentence = pos_tagging(sentence)\n",
        "    for word in pos_tagged_sentence:\n",
        "        if word.lower() not in Stopwords and word not in Stopwords and len(word)>1: \n",
        "            word = word.lower()\n",
        "            word = wordlemmatizer.lemmatize(word)\n",
        "            sentence_score = sentence_score + word_tfidf(dict_freq,word,sentences,sentence)\n",
        "    return sentence_score"
      ],
      "metadata": {
        "id": "Y91QRVxaRYwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Dataset : An artcle from Google News"
      ],
      "metadata": {
        "id": "0oi1EmstYlgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('/content/sample.txt' , 'r')\n",
        "text = file.read()\n",
        "tokenized_sentence = sent_tokenize(text)\n",
        "text = remove_special_characters(str(text))\n",
        "text = re.sub(r'\\d+', '', text) "
      ],
      "metadata": {
        "id": "PcTNleQDRa0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEJV2dbXTtms",
        "outputId": "4d764edf-bb47-407c-e10a-b8b274679d65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Percentage of information to retain:\n",
        "\n",
        "I took 30% in my summary"
      ],
      "metadata": {
        "id": "7j4vOWYZYbe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_words_with_stopwords = word_tokenize(text)\n",
        "tokenized_words = [word for word in tokenized_words_with_stopwords if word not in Stopwords]\n",
        "tokenized_words = [word for word in tokenized_words if len(word) > 1]\n",
        "tokenized_words = [word.lower() for word in tokenized_words]\n",
        "tokenized_words = lemmatize_words(tokenized_words)"
      ],
      "metadata": {
        "id": "hM0kN3ZkTqpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_freq = freq(tokenized_words)\n",
        "print(word_freq)\n",
        "input_user = int(input('Percentage of information to retain(in percent):'))\n",
        "no_of_sentences = int((input_user * len(tokenized_sentence))/100)\n",
        "print(no_of_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CNPnsnzTzUv",
        "outputId": "6f1ca3fa-a389-46d6-ca4a-1733aa82745f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'after': 1, 'month': 1, 'drama': 1, 'testy': 1, 'statement': 1, 'bit': 1, 'trolling': 1, 'musk': 10, 'reveal': 1, 'juicy': 1, 'text': 1, 'message': 1, 'elon': 5, 'finally': 1, 'closing': 1, 'deal': 4, 'purchase': 2, 'twitter': 11, 'billion': 1, 'now': 1, 'completed': 1, 'according': 1, 'report': 1, 'already': 1, 'gone': 1, 'senior': 1, 'leadership': 2, 'it': 1, 'wellknown': 1, 'monthslong': 1, 'saga': 1, 'love': 1, 'lost': 1, 'ceo': 3, 'parag': 1, 'agrawal': 6, 'also': 1, 'doesnt': 1, 'like': 2, 'top': 1, 'leader': 1, 'including': 1, 'company': 3, 'policy': 1, 'chief': 1, 'vijaya': 1, 'gadde': 2, 'cfo': 1, 'ned': 1, 'segal': 1, 'so': 1, 'three': 1, 'fired': 1, 'probably': 1, 'along': 1, 'at': 1, 'first': 1, 'glance': 1, 'look': 2, 'win': 1, 'sparred': 1, 'almost': 1, 'public': 1, 'privately': 1, 'since': 1, 'launched': 1, 'bid': 1, 'get': 1, 'stake': 1, 'but': 2, 'closer': 1, 'clear': 1, 'winner': 1, 'tussle': 1, 'sure': 1, 'losing': 1, 'job': 1, 'significantly': 1, 'even': 2, 'loses': 1, 'something': 2, 'gaining': 1, 'lot': 1, 'forced': 2, 'go': 2, 'extremely': 1, 'lucrative': 1, 'shareholder': 1, 'keep': 1, 'mind': 1, 'tried': 1, 'walk': 1, 'away': 1, 'personally': 1, 'getting': 1, 'firing': 1, 'when': 1, 'became': 1, 'last': 1, 'year': 2, 'december': 1, 'clause': 1, 'contract': 1, 'guaranteed': 1, 'severance': 1, 'fee': 1, 'million': 1, 'asked': 1, 'one': 1, 'this': 1, 'money': 1, 'pay': 1}\n",
            "Percentage of information to retain(in percent):70\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c = 1\n",
        "sentence_with_importance = {} #making a dictionary of the most important sentences\n",
        "for sent in tokenized_sentence:\n",
        "    sentenceimp = sentence_importance(sent,word_freq,tokenized_sentence)\n",
        "    sentence_with_importance[c] = sentenceimp\n",
        "    c = c+1\n",
        "sentence_with_importance = sorted(sentence_with_importance.items(), key=operator.itemgetter(1),reverse=True)\n"
      ],
      "metadata": {
        "id": "cfzYQcj4T6xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnt = 0\n",
        "sentence_no = []\n",
        "for word_prob in sentence_with_importance:\n",
        "    if cnt < no_of_sentences:\n",
        "        sentence_no.append(word_prob[0])\n",
        "        cnt = cnt+1\n",
        "    else:\n",
        "        break\n",
        "sentence_no.sort()\n"
      ],
      "metadata": {
        "id": "-7gF-mFLUUd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 1\n",
        "summary=\"\"\n",
        "for sentence in tokenized_sentence:\n",
        "    if count in sentence_no:\n",
        "        summary+= sentence+\" \"\n",
        "    count+=1\n",
        "print(\"\\n\")\n",
        "print(\"Summary:\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwoOAcNbUZoG",
        "outputId": "d74423fd-873c-45ed-ddc3-19969e9bb780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Summary:\n",
            "After months of drama, some testy statements, a bit of trolling from Musk and the reveal of some juicy text messages, Elon Musk is finally closing his deal to purchase Twitter for some $44 billion. Now as he completed the deal, according to reports, Musk has already gone after the senior leadership of Twitter. It is well-known after the months-long saga that there is no love lost between Elon Musk and Twitter CEO Parag Agrawal. At first glance it looks like a \"win\" for Elon Musk, who has sparred -- almost in public with Gadde and privately with Agrawal -- with Twitter leadership since he launched his bid to get a stake in the company. Sure, he is losing his job as the CEO of Twitter. Keep in mind that Agrawal has forced him to purchase Twitter even when Musk tried to walk away from the deal. Personally too, Agrawal is getting something out of his firing. When he became the CEO of Twitter last year in December, he did so with a clause in his contract that guaranteed him a severance fee of $42 million if Twitter asked him to go before one year was over. This is the money that Twitter, now an Elon Musk company, will pay to Agrawal. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TextRank Algorithm"
      ],
      "metadata": {
        "id": "RhxGH4g0KuaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVe word embeddings are vector representation of words. These word embeddings will be used to create vectors for our sentences. We could have also used the Bag-of-Words or TF-IDF approaches to create features for our sentences, but these methods ignore the order of the words (and the number of features is usually pretty large)."
      ],
      "metadata": {
        "id": "Nar84ZCDb1uX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mfn5LMk5a_iI",
        "outputId": "5ce94e99-24b1-4084-d907-2129ea36d68e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-29 05:40:03--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-10-29 05:40:03--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-10-29 05:40:03--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
            "\n",
            "2022-10-29 05:42:42 (5.19 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Word Embeddings using Glove\n",
        "word_embeddings = {}\n",
        "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    word_embeddings[word] = coefs\n",
        "f.close()\n"
      ],
      "metadata": {
        "id": "hhP-VAA8b2_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Vectors:\n",
        "sentence_vectors = []\n",
        "for i in tokenized_sentence:\n",
        "  \n",
        "  v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split()))\n",
        "  sentence_vectors.append(v)"
      ],
      "metadata": {
        "id": "ww3ms-2ZeZZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarity Matrix and Cosine Similarity:"
      ],
      "metadata": {
        "id": "msxRz0orZEgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# similarity matrix\n",
        "sim_mat = np.zeros([len(tokenized_sentence), len(tokenized_sentence)])"
      ],
      "metadata": {
        "id": "uocgD1UueoQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "A-eM9zxXe2f2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(tokenized_sentence)):\n",
        "  for j in range(len(tokenized_sentence)):\n",
        "    if i != j:\n",
        "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))"
      ],
      "metadata": {
        "id": "NkrhuaSte3V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scores:"
      ],
      "metadata": {
        "id": "6vpQMRdvZH5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "nx_graph = nx.from_numpy_array(sim_mat)\n",
        "scores = nx.pagerank(nx_graph)\n",
        "print(scores)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcC-mLyQfAkf",
        "outputId": "543058c3-0447-4fa1-a1ab-b903b7969960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 0.07720467818775745, 1: 0.07734935189459856, 2: 0.07668964819450208, 3: 0.07489964726247701, 4: 0.07626792848046823, 5: 0.07772990681380341, 6: 0.07733549841157401, 7: 0.07705044704704528, 8: 0.07781480820247667, 9: 0.0768813022045021, 10: 0.07638155985670204, 11: 0.0777200849204418, 12: 0.07667513852365136}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scoresrank =  sorted(scores.items(), key=operator.itemgetter(1),reverse=True)\n",
        "print(scoresrank)"
      ],
      "metadata": {
        "id": "eLoxYCGxJd8G",
        "outputId": "b6ce559b-c7fe-4373-bc02-812cf60ad08e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(8, 0.07781480820247667), (5, 0.07772990681380341), (11, 0.0777200849204418), (1, 0.07734935189459856), (6, 0.07733549841157401), (0, 0.07720467818775745), (7, 0.07705044704704528), (9, 0.0768813022045021), (2, 0.07668964819450208), (12, 0.07667513852365136), (10, 0.07638155985670204), (4, 0.07626792848046823), (3, 0.07489964726247701)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify number of sentences to form the summary\n",
        "sn = 4\n",
        "# Generate summary\n",
        "for i in range(sn):\n",
        "  print(tokenized_sentence[scoresrank[i][0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie1UA_Vgfjuc",
        "outputId": "91927e0a-274d-4b6a-b18c-3ab08804bc26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "But significantly, even as he loses something, he is gaining a lot because he has forced Elon Musk to go through a deal that is extremely lucrative for Twitter shareholders.\n",
            "At first glance it looks like a \"win\" for Elon Musk, who has sparred -- almost in public with Gadde and privately with Agrawal -- with Twitter leadership since he launched his bid to get a stake in the company.\n",
            "When he became the CEO of Twitter last year in December, he did so with a clause in his contract that guaranteed him a severance fee of $42 million if Twitter asked him to go before one year was over.\n",
            "Now as he completed the deal, according to reports, Musk has already gone after the senior leadership of Twitter.\n"
          ]
        }
      ]
    }
  ]
}